{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Article Webscraper\n",
    "This notebook contains code that webscrapes [medium.com](https://medium.com/) in order to gain key variables about articles written under the 'Data Science' tag. This webscraper scrapes the following variables related to an article:\n",
    "- __title__: Title of the article\n",
    "- __article_url__: URL link to the article\n",
    "- __claps__: The number of claps\n",
    "- __reading_time__: The time it takes to read the article in minutes\n",
    "- __date__: The date that the article was published \n",
    "- __tag_list__: Other tags that the article uses other than 'Data Science'\n",
    "\n",
    "\n",
    "\n",
    "_Sources for this code include:_\n",
    "\n",
    "(1) [Harrison Jansma](https://github.com/harrisonjansma/Medium_Scraper/blob/master/medium_scraper.py) (Apache License 2.0)\n",
    "\n",
    "(2) [Dorian Lazar](https://github.com/lazuxd/medium-scraping/blob/master/medium_scraping.ipynb) \n",
    "\n",
    "For more information on the use of this scraper refer to the [README.md](https://github.com/srpatel2000/DSA-Data-Vis.-Competition-Submission) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was taken directly from Harrison Jansma w/ some modification to the executable path\n",
    "def open_chrome():\n",
    "    \"\"\"Opens a chrome driver\"\"\"\n",
    "    driver = webdriver.Chrome(executable_path='/Users/siddhipatel/Downloads/chromedriver_mac64/chromedriver')\n",
    "    driver.implicitly_wait(30)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was taken directly from Dorian Lazar\n",
    "def get_claps(claps_str):\n",
    "    \"\"\"Gets the number of claps in an article\"\"\"\n",
    "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
    "        return 0\n",
    "    split = claps_str.split('K')\n",
    "    claps = float(split[0])\n",
    "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
    "    return claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-03 3 16\n",
      "2020-01-04 3 16\n",
      "2020-01-05 3 16\n",
      "2020-01-06 3 16\n",
      "2020-01-07 3 16\n",
      "2020-01-08 3 16\n",
      "2020-01-09 3 16\n",
      "2020-01-10 3 16\n",
      "2020-01-11 3 16\n",
      "2020-01-12 3 16\n",
      "2020-01-13 3 16\n",
      "2020-01-14 3 16\n",
      "2020-01-15 3 16\n",
      "2020-02-01 1 26\n",
      "2020-02-02 1 26\n",
      "2020-02-03 1 26\n",
      "2020-02-04 1 26\n",
      "2020-02-05 1 26\n",
      "2020-02-06 1 26\n",
      "2020-02-07 1 26\n",
      "2020-02-08 1 26\n",
      "2020-02-09 1 26\n",
      "2020-02-10 1 26\n",
      "2020-02-11 1 26\n",
      "2020-02-12 1 26\n",
      "2020-02-13 1 26\n",
      "2020-02-14 1 26\n",
      "2020-02-15 1 26\n",
      "2020-02-16 1 26\n",
      "2020-02-17 1 26\n",
      "2020-02-18 1 26\n",
      "2020-02-19 1 26\n",
      "2020-02-20 1 26\n",
      "2020-02-21 1 26\n",
      "2020-02-22 1 26\n",
      "2020-02-23 1 26\n",
      "2020-02-24 1 26\n",
      "2020-02-25 1 26\n",
      "2020-03-03 3 30\n",
      "2020-03-04 3 30\n",
      "2020-03-05 3 30\n",
      "2020-03-06 3 30\n",
      "2020-03-07 3 30\n",
      "2020-03-08 3 30\n",
      "2020-03-09 3 30\n",
      "2020-03-10 3 30\n",
      "2020-03-11 3 30\n",
      "2020-03-12 3 30\n",
      "2020-03-13 3 30\n",
      "2020-03-14 3 30\n",
      "2020-03-15 3 30\n",
      "2020-03-16 3 30\n",
      "2020-03-17 3 30\n",
      "2020-03-18 3 30\n",
      "2020-03-19 3 30\n",
      "2020-03-20 3 30\n",
      "2020-03-21 3 30\n",
      "2020-03-22 3 30\n",
      "2020-03-23 3 30\n",
      "2020-03-24 3 30\n",
      "2020-03-25 3 30\n",
      "2020-03-26 3 30\n",
      "2020-03-27 3 30\n",
      "2020-03-28 3 30\n",
      "2020-03-29 3 30\n",
      "2020-04-01 1 20\n",
      "2020-04-02 1 20\n",
      "2020-04-03 1 20\n",
      "2020-04-04 1 20\n",
      "2020-04-05 1 20\n",
      "2020-04-06 1 20\n",
      "2020-04-07 1 20\n",
      "2020-04-08 1 20\n",
      "2020-04-09 1 20\n",
      "2020-04-10 1 20\n",
      "2020-04-11 1 20\n",
      "2020-04-12 1 20\n",
      "2020-04-13 1 20\n",
      "2020-04-14 1 20\n",
      "2020-04-15 1 20\n",
      "2020-04-16 1 20\n",
      "2020-04-17 1 20\n",
      "2020-04-18 1 20\n",
      "2020-04-19 1 20\n",
      "2020-05-04 4 30\n",
      "2020-05-05 4 30\n",
      "2020-05-06 4 30\n",
      "2020-05-07 4 30\n",
      "2020-05-08 4 30\n",
      "2020-05-09 4 30\n",
      "2020-05-10 4 30\n",
      "2020-05-11 4 30\n",
      "2020-05-12 4 30\n",
      "2020-05-13 4 30\n",
      "2020-05-14 4 30\n",
      "2020-05-15 4 30\n",
      "2020-05-16 4 30\n",
      "2020-05-17 4 30\n",
      "2020-05-18 4 30\n",
      "2020-05-19 4 30\n",
      "2020-05-20 4 30\n",
      "2020-05-21 4 30\n",
      "2020-05-22 4 30\n",
      "2020-05-23 4 30\n",
      "2020-05-24 4 30\n",
      "2020-05-25 4 30\n",
      "2020-05-26 4 30\n",
      "2020-05-27 4 30\n",
      "2020-05-28 4 30\n",
      "2020-05-29 4 30\n",
      "2020-06-08 8 27\n",
      "2020-06-09 8 27\n",
      "2020-06-10 8 27\n",
      "2020-06-11 8 27\n",
      "2020-06-12 8 27\n",
      "2020-06-13 8 27\n",
      "2020-06-14 8 27\n",
      "2020-06-15 8 27\n",
      "2020-06-16 8 27\n",
      "2020-06-17 8 27\n",
      "2020-06-18 8 27\n",
      "2020-06-19 8 27\n",
      "2020-06-20 8 27\n",
      "2020-06-21 8 27\n",
      "2020-06-22 8 27\n",
      "2020-06-23 8 27\n",
      "2020-06-24 8 27\n",
      "2020-06-25 8 27\n",
      "2020-06-26 8 27\n",
      "2020-07-02 2 18\n",
      "2020-07-03 2 18\n",
      "2020-07-04 2 18\n",
      "2020-07-05 2 18\n",
      "2020-07-06 2 18\n",
      "2020-07-07 2 18\n",
      "2020-07-08 2 18\n",
      "2020-07-09 2 18\n",
      "2020-07-10 2 18\n",
      "2020-07-11 2 18\n",
      "2020-07-12 2 18\n",
      "2020-07-13 2 18\n",
      "2020-07-14 2 18\n",
      "2020-07-15 2 18\n",
      "2020-07-16 2 18\n",
      "2020-07-17 2 18\n",
      "2020-08-11 11 20\n",
      "2020-08-12 11 20\n",
      "2020-08-13 11 20\n",
      "2020-08-14 11 20\n",
      "2020-08-15 11 20\n",
      "2020-08-16 11 20\n",
      "2020-08-17 11 20\n",
      "2020-08-18 11 20\n",
      "2020-08-19 11 20\n"
     ]
    }
   ],
   "source": [
    "# main script that scrapes the articles\n",
    "\n",
    "data = [] # stores the data gathered by the scraper\n",
    "year = 2020 # gathers data only in 2020\n",
    "month = 1 # starts on the month of January \n",
    "num_days = [31, 29, 31, 30, 31, 30, 31, 27] # gets the number of days every month of 2020\n",
    "url = 'https://medium.com/tag/data-science/archive/{0}/{1:02d}/{2:02d}' # base url \n",
    "\n",
    "for days in num_days:\n",
    "    chrome_driver = open_chrome()\n",
    "    start_date = np.random.randint(1, (days/2)-1) # random start date\n",
    "    end_date = np.random.randint(days/2, days) # random end date\n",
    "    for i in range(start_date, end_date):\n",
    "        date = '{0}-{1:02d}-{2:02d}'.format(year, month, i)\n",
    "        print(date + \" \" + str(start_date) + \" \" + str(end_date))\n",
    "        \n",
    "        response = chrome_driver.get(url.format(year, month, i))\n",
    "        strainer = SoupStrainer('div')\n",
    "        #gathers all the articles in a specific day of the month\n",
    "        soup = bs4.BeautifulSoup(chrome_driver.page_source, 'lxml', parse_only=strainer)\n",
    "        articles = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
    "        \n",
    "        for article in articles:\n",
    "            # grabs the title of the article\n",
    "            title = article.find(\"h3\", class_=\"graf--title\")\n",
    "            if title is None:\n",
    "                continue\n",
    "            title = title.contents[0]\n",
    "            \n",
    "            # grabs the article url\n",
    "            article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
    "            \n",
    "            # grabs the number of claps in a given article\n",
    "            claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
    "            \n",
    "            # grabs the reading time of a given article\n",
    "            reading_time = article.find(\"span\", class_=\"readingTime\")\n",
    "            reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
    "            \n",
    "            # grabs the related tags under an article\n",
    "            response = chrome_driver.get(article_url)\n",
    "            strainer = SoupStrainer('ul')\n",
    "            tags_soup = bs4.BeautifulSoup(chrome_driver.page_source, 'lxml', parse_only=strainer)\n",
    "            try:\n",
    "                tags = tags_soup.find_all(\"ul\")[len(tags_soup.find_all(\"ul\"))-1].findChildren(\"a\")\n",
    "            except:\n",
    "                tags = None\n",
    "            if tags is None:\n",
    "                continue\n",
    "            tag_list = []\n",
    "            for i in range(len(tags)):\n",
    "                tag_list.append(tags[i].contents[0])\n",
    "            # if Data Science isn't a tag that means the article was only for paid members\n",
    "            # so the article was skipped and the data was not inputted into the table\n",
    "            if 'Data Science' not in tag_list:\n",
    "                continue\n",
    "                \n",
    "            time.sleep(2)\n",
    "            \n",
    "            data.append([title, article_url, claps, reading_time, date, tag_list])\n",
    "    time.sleep(10)\n",
    "    # moves onto the next month\n",
    "    month += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article_url</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>date</th>\n",
       "      <th>tag_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 10 Technology Trends for 2020</td>\n",
       "      <td>https://towardsdatascience.com/top-10-technolo...</td>\n",
       "      <td>3000</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>[Technology, Trends, Artificial Intelligence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 10 Skills for a Data Scientist</td>\n",
       "      <td>https://towardsdatascience.com/top-10-skills-f...</td>\n",
       "      <td>2200</td>\n",
       "      <td>9</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>[Data Science, Technology, Business, Machine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML Ops: Machine Learning as an Engineering Dis...</td>\n",
       "      <td>https://towardsdatascience.com/ml-ops-machine-...</td>\n",
       "      <td>1300</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>[Data Science, Machine Learning, Data Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Organizing your Python Code</td>\n",
       "      <td>https://medium.com/@k3no/organizing-your-pytho...</td>\n",
       "      <td>1200</td>\n",
       "      <td>13</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>[Python, Programming, Data Science, Coding, So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to be fancy with OOP in Python</td>\n",
       "      <td>https://towardsdatascience.com/how-to-be-fancy...</td>\n",
       "      <td>928</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>[Programming, Python, Data Science, Coding]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                  Top 10 Technology Trends for 2020   \n",
       "1                 Top 10 Skills for a Data Scientist   \n",
       "2  ML Ops: Machine Learning as an Engineering Dis...   \n",
       "3                        Organizing your Python Code   \n",
       "4                 How to be fancy with OOP in Python   \n",
       "\n",
       "                                         article_url  claps  reading_time  \\\n",
       "0  https://towardsdatascience.com/top-10-technolo...   3000            10   \n",
       "1  https://towardsdatascience.com/top-10-skills-f...   2200             9   \n",
       "2  https://towardsdatascience.com/ml-ops-machine-...   1300            10   \n",
       "3  https://medium.com/@k3no/organizing-your-pytho...   1200            13   \n",
       "4  https://towardsdatascience.com/how-to-be-fancy...    928             3   \n",
       "\n",
       "         date                                           tag_list  \n",
       "0  2020-01-03  [Technology, Trends, Artificial Intelligence, ...  \n",
       "1  2020-01-03  [Data Science, Technology, Business, Machine L...  \n",
       "2  2020-01-03  [Data Science, Machine Learning, Data Engineer...  \n",
       "3  2020-01-03  [Python, Programming, Data Science, Coding, So...  \n",
       "4  2020-01-03        [Programming, Python, Data Science, Coding]  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_data = pd.DataFrame(data, columns = ['title', 'article_url', 'claps', 'reading_time', 'date', 'tag_list'])\n",
    "medium_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_data.to_csv('medium_articles.csv', header=True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
